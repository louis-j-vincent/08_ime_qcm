{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0c2b9d",
   "metadata": {},
   "source": [
    "For each QCM question, we have ONE response and need to generate 3 other valid (but false) answers from the same category, which we will call distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad355ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5ea1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a423785",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "from qcmgen.qcm import generate_payloads, _normalize_answer, build_choices_with_arasaac, build_choices, QuestionType\n",
    "from qcmgen.nlp import extract_facts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0783223a",
   "metadata": {},
   "source": [
    "## Strategy with no LLM call\n",
    "\n",
    "When we're not using a LLM to generate the answers, we extract the relevant information from the *generate_payloads* function. \n",
    "\n",
    "For each generated payload, we can generate a question, get the correct answer, and create the choices (with the 3 other distractors) via the *build_choices_with_arasaac* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f18a558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Quel animal est noir ?\n",
      "Correct answer: chat\n",
      "Choices: chaton\n"
     ]
    }
   ],
   "source": [
    "facts = extract_facts(\"Le chat noir de Victor dort sur le canapé.\")\n",
    "\n",
    "for fact in facts:\n",
    "    payloads = generate_payloads(fact)\n",
    "    for payload in payloads:\n",
    "        # generate question text\n",
    "        question = payload.template.format(**payload.template_vars) #remplir les variables du template (ex : \"Que {verb} {subj} ?\".format(verb=\"voir\", subj=\"Martin\") => \"Que voir Martin ?\") \n",
    "        correct = _normalize_answer(payload.correct)\n",
    "        if payload.qtype in (QuestionType.OBJECT, QuestionType.ADJ_NOUN):\n",
    "            choices, answer_index = build_choices_with_arasaac(correct, k=3)\n",
    "        else:\n",
    "            choices, answer_index = build_choices(correct, payload.pool_name, k=3) \n",
    "\n",
    "\n",
    "        print(\"Question:\", question)\n",
    "        print(\"Correct answer:\", correct)\n",
    "        print(\"Choices:\", choices[0])\n",
    "\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943f5bf",
   "metadata": {},
   "source": [
    "However, this logic is not robust and requires a lot of hardcoded rules, so it works well in a controled environment where questions all share the same structure, but doesn't extend well to more general type questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1e6fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No payloads could be generated for this fact\n"
     ]
    }
   ],
   "source": [
    "facts = extract_facts(\"Jeremie et son frère vont au supermarché acheter des pommes\")\n",
    "\n",
    "for fact in facts:\n",
    "    payloads = generate_payloads(fact)\n",
    "\n",
    "    if len(payloads) == 0:\n",
    "        print(\"No payloads could be generated for this fact\")\n",
    "        continue\n",
    "\n",
    "    for payload in payloads:\n",
    "        # generate question text\n",
    "        question = payload.template.format(**payload.template_vars) #remplir les variables du template (ex : \"Que {verb} {subj} ?\".format(verb=\"voir\", subj=\"Martin\") => \"Que voir Martin ?\") \n",
    "        correct = _normalize_answer(payload.correct)\n",
    "        if payload.qtype in (QuestionType.OBJECT, QuestionType.ADJ_NOUN):\n",
    "            choices, answer_index = build_choices_with_arasaac(correct, k=3)\n",
    "        else:\n",
    "            choices, answer_index = build_choices(correct, payload.pool_name, k=3)        \n",
    "\n",
    "        print(\"Question:\", question)\n",
    "        print(\"Correct answer:\", correct)\n",
    "        print(\"Choices:\", choices[0])\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b79d48",
   "metadata": {},
   "source": [
    "## Let's make a more robust version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51612a49",
   "metadata": {},
   "source": [
    "#### We start by setting a list of categories\n",
    "\n",
    "For each of these categories, we'll call the arasaac api to generate a list of distractors which we will put in the cache, to avoid costly api calls later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f2a199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    \"animal\": [\n",
    "        \"chat\", \"chien\", \"cheval\", \"vache\", \"mouton\", \"cochon\",\n",
    "        \"lapin\", \"souris\", \"oiseau\", \"poisson\", \"lion\", \"éléphant\"\n",
    "    ],\n",
    "    \"food\": [\n",
    "        \"pomme\", \"banane\", \"poire\", \"pain\", \"fromage\", \"gâteau\",\n",
    "        \"riz\", \"pâtes\", \"soupe\", \"chocolat\", \"yaourt\"\n",
    "    ],\n",
    "    \"drink\": [\n",
    "        \"eau\", \"lait\", \"jus\", \"café\", \"thé\", \"chocolat chaud\"\n",
    "    ],\n",
    "    \"furniture\": [\n",
    "        \"table\", \"chaise\", \"lit\", \"canapé\", \"armoire\", \"bureau\", \"étagère\"\n",
    "    ],\n",
    "    \"clothing\": [\n",
    "        \"pantalon\", \"tee-shirt\", \"robe\", \"pull\", \"manteau\",\n",
    "        \"chaussures\", \"chaussettes\", \"chapeau\"\n",
    "    ],\n",
    "    \"profession\": [\n",
    "        \"médecin\", \"infirmier\", \"enseignant\", \"policier\",\n",
    "        \"pompier\", \"boulanger\", \"cuisinier\", \"chauffeur\"\n",
    "    ],\n",
    "    \"sport\": [\n",
    "        \"football\", \"basket\", \"tennis\", \"natation\",\n",
    "        \"vélo\", \"course\", \"judo\", \"gymnastique\"\n",
    "    ],\n",
    "    \"musical_instrument\": [\n",
    "        \"piano\", \"guitare\", \"violon\", \"tambour\", \"trompette\", \"flûte\"\n",
    "    ],\n",
    "    \"color\": [\n",
    "        \"rouge\", \"bleu\", \"vert\", \"jaune\", \"rose\",\n",
    "        \"noir\", \"blanc\", \"gris\", \"orange\"\n",
    "    ],\n",
    "    \"shape\": [\n",
    "        \"rond\", \"carré\", \"triangle\", \"rectangle\", \"étoile\", \"cœur\"\n",
    "    ],\n",
    "    \"weather\": [\n",
    "        \"soleil\", \"pluie\", \"neige\", \"vent\", \"nuage\", \"orage\"\n",
    "    ],\n",
    "    \"emotion\": [\n",
    "        \"content\", \"triste\", \"en colère\", \"peur\", \"surpris\", \"fatigué\"\n",
    "    ],\n",
    "    \"body_part\": [\n",
    "        \"tête\", \"main\", \"pied\", \"bras\", \"jambe\", \"œil\", \"bouche\", \"nez\"\n",
    "    ],\n",
    "    \"vehicle\": [\n",
    "        \"voiture\", \"vélo\", \"bus\", \"camion\", \"train\", \"moto\", \"avion\"\n",
    "    ],\n",
    "    \"place\": [\n",
    "        \"maison\", \"école\", \"parc\", \"hôpital\", \"magasin\", \"piscine\", \"rue\"\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "becbe5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching pictograms for distractor categories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:03<00:00,  3.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from qcmgen.pictos.resolve import resolve_term_to_picto_strict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "arasaac_cache_path = project_root / \"data\" / \"arasaac_cache_fr.json\"\n",
    "\n",
    "with open(arasaac_cache_path) as f:\n",
    "    cached_pictos = json.load(f)\n",
    "\n",
    "print('Caching pictograms for distractor categories...')\n",
    "for category in tqdm(CATEGORIES):\n",
    "    for word in CATEGORIES[category]:\n",
    "        # check if picto is in cache, if not fetch from arasaac and store in cache\n",
    "        if not word in cached_pictos:\n",
    "            resolved_picto = resolve_term_to_picto_strict(word, expected_type=category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf75763",
   "metadata": {},
   "source": [
    " You can directly run *python scripts/build_cache_from_categories.py* from the root of the repo to achieve the same result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c6f4f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "load_dotenv(\".env\")  # loads from .env in OpenAI directory\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519a3fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Responses.create() got an unexpected keyword argument 'response_format'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m llm_prompt = \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mllm_prompt.txt\u001b[39m\u001b[33m\"\u001b[39m).read().strip()\n\u001b[32m      3\u001b[39m sentences = [\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPapa a bu son café avec du sucre.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLe chat noir dort avec les chatons.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m resp = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m- \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ms\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Responses.create() got an unexpected keyword argument 'response_format'"
     ]
    }
   ],
   "source": [
    "llm_prompt = open(\"llm_prompt.txt\").read().strip()\n",
    "\n",
    "sentences = [\n",
    "    \"Papa a bu son café avec du sucre.\",\n",
    "    \"Le chat noir dort avec les chatons.\"\n",
    "]\n",
    "\n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=llm_prompt,\n",
    "    input=\"\\n\".join(f\"- {s}\" for s in sentences),\n",
    "    #response_format={\"type\": \"json\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6bcc2201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Qu'est-ce que papa a bu ?\",\n",
       " 'answer': 'Café',\n",
       " 'category': 'drink'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfc63e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Qu'est-ce que papa a bu ?\n",
      "Correct answer: Café\n",
      "Question category: drink\n",
      "Distractors: ['café', 'jus', 'thé']\n",
      "\n",
      "Question: Qu'est-ce qu'il y a dans le café ?\n",
      "Correct answer: Sucre\n",
      "Question category: food\n",
      "Distractors: ['soupe', 'riz', 'pomme']\n",
      "\n",
      "Question: De quelle couleur est le chat ?\n",
      "Correct answer: Noir\n",
      "Question category: color\n",
      "Distractors: ['rouge', 'gris', 'vert']\n",
      "\n",
      "Question: Avec qui le chat dort-il ?\n",
      "Correct answer: Les chatons\n",
      "Question category: animal\n",
      "Distractors: ['oiseau', 'lion', 'poisson']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = resp.output_text\n",
    "\n",
    "clean = raw.strip()\n",
    "start = raw.find(\"[\")\n",
    "end = raw.rfind(\"]\") + 1\n",
    "\n",
    "clean = raw[start:end]\n",
    "\n",
    "output_json = json.loads(clean)\n",
    "\n",
    "from random import sample\n",
    "\n",
    "for item in output_json:\n",
    "    for question in item[\"questions\"]:\n",
    "        print(\"Question:\", question[\"question\"])\n",
    "        print(\"Correct answer:\", question[\"answer\"])\n",
    "        print(\"Question category:\", question[\"category\"])\n",
    "        # sample 3 random distractors from category\n",
    "        distractor_candidates = [elt for elt in CATEGORIES[question[\"category\"]] if elt != question[\"answer\"]]\n",
    "        distractors = sample(distractor_candidates, k = 3)\n",
    "        print(\"Distractors:\", distractors)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e55a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e4f503e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ime-qcm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
